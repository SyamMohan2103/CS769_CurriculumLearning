{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/cs769_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "# from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset, load_from_disk\n",
    "import random\n",
    "import os\n",
    "import evaluate\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value) \n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"cs769_llama\"\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_mcq_input(question, option_a, option_b, option_c, option_d, correct_option):\n",
    "    \"\"\"\n",
    "    Format the MCQ question and options into a single text string for the model.\n",
    "    \n",
    "    Parameters:\n",
    "    - question: The question text\n",
    "    - option_a, option_b, option_c, option_d: The option texts\n",
    "    - correct_option: The correct option (A, B, C, or D)\n",
    "    \n",
    "    Returns:\n",
    "    - A formatted string combining all information\n",
    "    \"\"\"\n",
    "\n",
    "    idx_to_ans_map = {0:\"A\", 1:\"B\", 2:\"C\", 3:\"D\"}\n",
    "\n",
    "\n",
    "    formatted_text = f\"Question: {question}\\n\"\n",
    "    # formatted_text += f\"A: {option_a}\\n\"\n",
    "    # formatted_text += f\"B: {option_b}\\n\"\n",
    "    # formatted_text += f\"C: {option_c}\\n\"\n",
    "    # formatted_text += f\"D: {option_d}\\n\"\n",
    "    # formatted_text += f\"Answer: {idx_to_ans_map[correct_option]}\"\n",
    "    # formatted_text += f\"Answer: A\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Preprocess function to format and tokenize the input examples.\n",
    "    \"\"\"\n",
    "    formatted_inputs = [\n",
    "        format_mcq_input(\n",
    "            question, opa, opb, opc, opd, cop\n",
    "        ) for question, opa, opb, opc, opd,cop in zip(\n",
    "            examples['question'], \n",
    "            examples['opa'], \n",
    "            examples['opb'], \n",
    "            examples['opc'], \n",
    "            examples['opd'],\n",
    "            examples['cop']\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Tokenize the formatted inputs\n",
    "    tokenized_inputs = tokenizer(\n",
    "        formatted_inputs,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return tokenized_inputs\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for evaluation.\n",
    "    \"\"\"\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "    result = {**accuracy}\n",
    "\n",
    "    return result\n",
    "\n",
    "def predict_difficulty(model, tokenizer, question, option_a, option_b, option_c, option_d):\n",
    "    \"\"\"\n",
    "    Predict the difficulty of a single MCQ question.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The trained model\n",
    "    - tokenizer: The tokenizer\n",
    "    - question, option_a, option_b, option_c, option_d, correct_option: MCQ components\n",
    "    \n",
    "    Returns:\n",
    "    - Predicted difficulty level ('easy', 'medium', or 'hard')\n",
    "    \"\"\"\n",
    "    # Format the input\n",
    "    formatted_input = format_mcq_input(\n",
    "        question, option_a, option_b, option_c, option_d\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        formatted_input,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    device = model.device\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Map prediction to difficulty level\n",
    "    difficulty_map = {0: 'easy', 1: 'medium', 2: 'hard'}\n",
    "    predicted_difficulty = difficulty_map[predictions.item()]\n",
    "    \n",
    "    return predicted_difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 127975\n",
      "Validation samples: 16454\n",
      "Testing samples: 38393\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "easy_data = load_from_disk('./json_to_hf/subset1')\n",
    "medium_data = load_from_disk('./json_to_hf/subset2')\n",
    "hard_data = load_from_disk('./json_to_hf/subset3')\n",
    "\n",
    "easy_df = pd.DataFrame(easy_data)\n",
    "medium_df = pd.DataFrame(medium_data)\n",
    "hard_df = pd.DataFrame(hard_data)\n",
    "\n",
    "# Add difficulty labels\n",
    "easy_df['difficulty'] = 'easy'\n",
    "medium_df['difficulty'] = 'medium'\n",
    "hard_df['difficulty'] = 'hard'\n",
    "\n",
    "# Combine dataframes\n",
    "combined_df = pd.concat([easy_df, medium_df, hard_df], ignore_index=True)\n",
    "\n",
    "# Shuffle the data\n",
    "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Map text labels to numeric\n",
    "label_map = {'easy': 0, 'medium': 1, 'hard': 2}\n",
    "combined_df['label'] = combined_df['difficulty'].map(label_map)\n",
    "\n",
    "# Split data\n",
    "train_df, temp_df = train_test_split(\n",
    "    combined_df, test_size=0.3, random_state=42, stratify=combined_df['difficulty']\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.7, random_state=42, stratify=temp_df['difficulty']\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Validation samples: {len(val_df)}\")\n",
    "print(f\"Testing samples: {len(test_df)}\")\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 127975/127975 [00:19<00:00, 6435.59 examples/s] \n",
      "Map (num_proc=4): 100%|██████████| 16454/16454 [00:03<00:00, 4942.04 examples/s]\n",
      "Map (num_proc=4): 100%|██████████| 38393/38393 [00:06<00:00, 5796.85 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "\n",
    "# Preprocess datasets\n",
    "train_dataset = train_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=['question', 'opa', 'opb', 'opc', 'opd', 'cop', 'difficulty', '__index_level_0__'],\n",
    "    num_proc=4,\n",
    "\n",
    ")\n",
    "val_dataset = val_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=['question', 'opa', 'opb', 'opc', 'opd', 'cop', 'difficulty', '__index_level_0__'],\n",
    "    num_proc=4,\n",
    ")\n",
    "test_dataset = test_dataset.map(\n",
    "    preprocess_function, \n",
    "    batched=True,\n",
    "    remove_columns=['question', 'opa', 'opb', 'opc', 'opd', 'cop', 'difficulty', '__index_level_0__'],\n",
    "    num_proc=4,\n",
    ")\n",
    "\n",
    "# Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_dir = './med_mcqa_router_deberta'\n",
    "# Initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'microsoft/deberta-base', \n",
    "    num_labels=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all encoder layers\n",
    "for param in model.deberta.encoder.layer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last two layers\n",
    "for layer in model.deberta.encoder.layer[-2:]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Also unfreeze the classifier head\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable layers:\n",
      "['deberta.embeddings.word_embeddings.weight', 'deberta.embeddings.LayerNorm.weight', 'deberta.embeddings.LayerNorm.bias', 'deberta.encoder.layer.10.attention.self.q_bias', 'deberta.encoder.layer.10.attention.self.v_bias', 'deberta.encoder.layer.10.attention.self.in_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.10.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.10.attention.output.dense.weight', 'deberta.encoder.layer.10.attention.output.dense.bias', 'deberta.encoder.layer.10.attention.output.LayerNorm.weight', 'deberta.encoder.layer.10.attention.output.LayerNorm.bias', 'deberta.encoder.layer.10.intermediate.dense.weight', 'deberta.encoder.layer.10.intermediate.dense.bias', 'deberta.encoder.layer.10.output.dense.weight', 'deberta.encoder.layer.10.output.dense.bias', 'deberta.encoder.layer.10.output.LayerNorm.weight', 'deberta.encoder.layer.10.output.LayerNorm.bias', 'deberta.encoder.layer.11.attention.self.q_bias', 'deberta.encoder.layer.11.attention.self.v_bias', 'deberta.encoder.layer.11.attention.self.in_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_q_proj.weight', 'deberta.encoder.layer.11.attention.self.pos_q_proj.bias', 'deberta.encoder.layer.11.attention.output.dense.weight', 'deberta.encoder.layer.11.attention.output.dense.bias', 'deberta.encoder.layer.11.attention.output.LayerNorm.weight', 'deberta.encoder.layer.11.attention.output.LayerNorm.bias', 'deberta.encoder.layer.11.intermediate.dense.weight', 'deberta.encoder.layer.11.intermediate.dense.bias', 'deberta.encoder.layer.11.output.dense.weight', 'deberta.encoder.layer.11.output.dense.bias', 'deberta.encoder.layer.11.output.LayerNorm.weight', 'deberta.encoder.layer.11.output.LayerNorm.bias', 'deberta.encoder.rel_embeddings.weight', 'pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n"
     ]
    }
   ],
   "source": [
    "trainable_layers = [name for name, param in model.named_parameters() if param.requires_grad]\n",
    "print(f\"Trainable layers:\\n{trainable_layers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msyammohan2103\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/workspace/CS769/wandb/run-20250502_215252-sfo2q3q6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/syammohan2103/cs769_llama/runs/sfo2q3q6' target=\"_blank\">router_classifier</a></strong> to <a href='https://wandb.ai/syammohan2103/cs769_llama' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/syammohan2103/cs769_llama' target=\"_blank\">https://wandb.ai/syammohan2103/cs769_llama</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/syammohan2103/cs769_llama/runs/sfo2q3q6' target=\"_blank\">https://wandb.ai/syammohan2103/cs769_llama/runs/sfo2q3q6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='245' max='16000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  245/16000 04:15 < 4:36:31, 0.95 it/s, Epoch 0.06/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.065200</td>\n",
       "      <td>1.068372</td>\n",
       "      <td>0.401118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.062500</td>\n",
       "      <td>1.063062</td>\n",
       "      <td>0.386654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    logging_steps=100,\n",
    "    eval_steps=100,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    # metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    "    report_to='wandb',  # Disable wandb, tensorboard etc.\n",
    "    run_name='router_classifier'\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    # callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Training model...\")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "# Save model and tokenizer\n",
    "model_path = os.path.join(model_dir, 'best')\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(f\"Test results: {test_results}\")\n",
    "\n",
    "# Make prediction on a sample\n",
    "sample_idx = 0\n",
    "sample = test_df.iloc[sample_idx]\n",
    "\n",
    "predicted_difficulty = predict_difficulty(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    sample['question'],\n",
    "    sample['opa'],\n",
    "    sample['opb'],\n",
    "    sample['opc'],\n",
    "    sample['opd'],\n",
    "    sample['cop']\n",
    ")\n",
    "\n",
    "print(f\"\\nSample question: {sample['question']}\")\n",
    "print(f\"Actual difficulty: {sample['difficulty']}\")\n",
    "print(f\"Predicted difficulty: {predicted_difficulty}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training results\n",
    "train_history = trainer.state.log_history\n",
    "\n",
    "# Extract metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for entry in train_history:\n",
    "    if 'loss' in entry and 'step' in entry:\n",
    "        train_losses.append(entry['loss'])\n",
    "    if 'eval_loss' in entry:\n",
    "        val_losses.append(entry['eval_loss'])\n",
    "    if 'eval_accuracy' in entry:\n",
    "        val_accuracies.append(entry['eval_accuracy'])\n",
    "\n",
    "# Plot\n",
    "epochs = range(1, len(val_losses) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, val_losses, 'b-', label='Validation Loss')\n",
    "plt.plot(epochs, val_accuracies, 'r-', label='Validation Accuracy')\n",
    "plt.title('Validation Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs769_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
