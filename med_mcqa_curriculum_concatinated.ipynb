{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import bitsandbytes as bnb\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "from trl import setup_chat_format, SFTConfig, SFTTrainer\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"cs769_llama\"\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.2-3b-Instruct\"\n",
    "root_model_dir = \"Llama-3.2-3b-it-Open-medmcqa-baseline-curriculum\"\n",
    "dataset_name = 'openlifescienceai/medmcqa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model and tokenizer\n",
    "\n",
    "- setting the configurations for Q-LoRA using BitsAndBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        token=HF_TOKEN,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_data = load_from_disk('./json_to_hf/subset1')\n",
    "medium_data = load_from_disk('./json_to_hf/subset2')\n",
    "hard_data = load_from_disk('./json_to_hf/subset3')\n",
    "\n",
    "easy_data = easy_data.to_pandas()\n",
    "medium_data = medium_data.to_pandas()\n",
    "hard_data = hard_data.to_pandas()\n",
    "\n",
    "concatinated_dataset = pd.concat([easy_data, medium_data, hard_data])\n",
    "\n",
    "concatinated_dataset = Dataset.from_pandas(concatinated_dataset, preserve_index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val_data = load_dataset(dataset_name, split='validation', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(easy_data).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(val_data).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "\n",
    "    instruction = \"\"\"Answer the following multiple choice question by giving the most appropriate response. \n",
    "Answer should be one among [A, B, C, D].\"\"\"\n",
    "\n",
    "    idx_to_ans_map = {0:\"A\", 1:\"B\", 2:\"C\", 3:\"D\"}\n",
    "    \n",
    "\n",
    "    a = row['opa']\n",
    "    b = row['opb']\n",
    "    c = row['opc']\n",
    "    d = row['opd']\n",
    "\n",
    "    user_instruction = f\"\"\"Question: {row['question']}\n",
    "                A) {a}\n",
    "                B) {b}\n",
    "                C) {c}\n",
    "                D) {d}\n",
    "            \"\"\"\n",
    "\n",
    "    row_json = [{\"role\": \"system\", \"content\": instruction },\n",
    "               {\"role\": \"user\", \"content\": user_instruction },\n",
    "               {\"role\": \"assistant\", \"content\": idx_to_ans_map[row['cop']]}]\n",
    "    \n",
    "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def get_mapped_dataset(easy_data, format_chat_template):\n",
    "    easy_train_dataset = {col: [] for col in easy_data.column_names}\n",
    "    easy_train_dataset.update({'text':[]})\n",
    "\n",
    "    for data in easy_data:\n",
    "        transformed_example = format_chat_template(data)\n",
    "        for col in easy_train_dataset.keys():\n",
    "            easy_train_dataset[col].append(transformed_example[col])\n",
    "\n",
    "    easy_train_dataset = Dataset.from_dict(easy_train_dataset)\n",
    "\n",
    "    return easy_train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = val_data.map(format_chat_template)\n",
    "\n",
    "easy_train_dataset = get_mapped_dataset(easy_data, format_chat_template)\n",
    "medium_train_dataset = get_mapped_dataset(medium_data, format_chat_template)\n",
    "hard_train_dataset = get_mapped_dataset(hard_data, format_chat_template)\n",
    "\n",
    "easy_train_dataset['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the linear module names of the Base Model to train LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16 bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=modules\n",
    ")\n",
    "\n",
    "# https://huggingface.co/docs/trl/en/sft_trainer\n",
    "model = get_peft_model(base_model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model))\n",
    "print(model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    output_dir=root_model_dir,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=250,\n",
    "    logging_steps=1,\n",
    "    warmup_ratio=0.03,\n",
    "    logging_strategy='steps',\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    remove_unused_columns=True,\n",
    "    report_to='wandb',\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field='text',\n",
    "    label_names=[\"labels\"],\n",
    "\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(easy_train_dataset))\n",
    "print(len(medium_train_dataset))\n",
    "print(len(hard_train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments.output_dir = os.path.join(root_model_dir, 'easy')\n",
    "\n",
    "easy_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=easy_train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "easy_trainer.train()\n",
    "\n",
    "easy_trainer.save_model(os.path.join(root_model_dir, 'easy', 'best')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments.output_dir = os.path.join(root_model_dir, 'medium')\n",
    "\n",
    "easy_model = trainer.model\n",
    "\n",
    "medium_trainer = SFTTrainer(\n",
    "    model=easy_model,\n",
    "    train_dataset=medium_train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "medium_trainer.train()\n",
    "\n",
    "medium_trainer.save_model(os.path.join(root_model_dir, 'medium', 'best'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments.output_dir = os.path.join(root_model_dir, 'hard')\n",
    "\n",
    "medium_model = trainer.model\n",
    "\n",
    "hard_trainer = SFTTrainer(\n",
    "    model=medium_model,\n",
    "    train_dataset=hard_train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "hard_trainer.train()\n",
    "\n",
    "hard_trainer.save_model(os.path.join(root_model_dir, 'hard', 'best'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Peft Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"Llama-3.2-3b-it-Open-medmcqa-baseline/checkpoint-5500\"\n",
    "new_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load LoRA adapter\n",
    "trained_model = PeftModel.from_pretrained(new_base_model, checkpoint_path)\n",
    "trained_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting for inference, the format should not have the answer\n",
    "\n",
    "def format_chat_prompt_for_inference(row):\n",
    "    instruction = \"\"\"Answer the following multiple choice question by giving the most appropriate response. \n",
    "Answer should be one among [A, B, C, D].\"\"\"\n",
    "\n",
    "    a = row['opa']\n",
    "    b = row['opb']\n",
    "    c = row['opc']\n",
    "    d = row['opd']\n",
    "\n",
    "    user_instruction = f\"\"\"Question: {row['question']}\n",
    "                A) {a}\n",
    "                B) {b}\n",
    "                C) {c}\n",
    "                D) {d}\n",
    "            \"\"\"\n",
    "\n",
    "    # No assistant response!\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": instruction},\n",
    "        {\"role\": \"user\", \"content\": user_instruction}\n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(row_json, tokenize=False, add_generation_prompt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_row = {\n",
    "    \"question\": \"A 35-year-old man has sudden severe chest pain radiating to his back. What is the most likely diagnosis?\",\n",
    "    \"opa\": \"Myocardial infarction\",\n",
    "    \"opb\": \"Pulmonary embolism\",\n",
    "    \"opc\": \"Aortic dissection\",\n",
    "    \"opd\": \"Pneumothorax\",\n",
    "    \"cop\": 2  \n",
    "}\n",
    "\n",
    "prompt = format_chat_prompt_for_inference(sample_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors='pt', truncation=True, padding=True).to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = trained_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=1,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "predicted_answer = decoded.split(\"assistant\")[-1].strip()\n",
    "print(\"Predicted answer:\", predicted_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Accuracy: 0.7316\n",
    "# Validation Accuracy: 0.5802"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mcqa_accuracy(model, tokenizer, dataset, max_samples=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    idx_to_ans_map = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "\n",
    "    if max_samples:\n",
    "        dataset = dataset.select(range(min(max_samples, len(dataset))))\n",
    "\n",
    "    for row in dataset:\n",
    "        # Prepare the prompt\n",
    "        instruction = \"\"\"Answer the following multiple choice question by giving the most appropriate response. \n",
    "        Answer should be one among [A, B, C, D].\"\"\"\n",
    "\n",
    "        user_instruction = f\"\"\"Question: {row['question']}\n",
    "                A) {row['opa']}\n",
    "                B) {row['opb']}\n",
    "                C) {row['opc']}\n",
    "                D) {row['opd']}\n",
    "            \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": instruction},\n",
    "            {\"role\": \"user\", \"content\": user_instruction}\n",
    "        ]\n",
    "\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,  # Just want the answer token (A/B/C/D)\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "        pred_answer = decoded.split(\"assistant\")[-1].strip()[:1]  # Get the first character after \"assistant\"\n",
    "\n",
    "        correct_answer = idx_to_ans_map[row['cop']]\n",
    "        print(pred_answer, correct_answer)\n",
    "        if pred_answer == correct_answer:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    print(f\"Evaluated {total} samples\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mcqa_accuracy(trained_model, tokenizer, val_dataset, max_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Accuracy = 57%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs769",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
